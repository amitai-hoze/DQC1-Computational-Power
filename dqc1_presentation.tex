%this is the DQC1 presentation

\documentclass{article}
\usepackage{etex}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{braket}
\usepackage{tikz}
\usepackage{braids}
\usepackage{qcircuit}
\newtheorem{definition}{Definition}

\begin{document}
\title{DQC1 complexity class and Jones polynomials} 
\author{Ohad Barta}
\date{\today} 

\titlepage
\tableofcontents



\section{DQC1 class and DQC1-complete problems}
\subsection{DQC1 class definition  } 
\title{DQC1 class definition} 
DQC1 class is the class of decidable languages with algorithm $A$ such that:
\begin{itemize}
\item $A$ starts with one clean qubit in state $\ket{0}$, and $n$ qubits in the maximally mixed state
\item $A$ may perform any unitary operation
\item $A$ can only perform a measurement of the clean qubit at the end of the algorithm
\item $A$ has no access to a classical computer, so its not promised that $P \subset DQC1$ 
\item $A$ cannot be invoked many times in parallel
\item $A$ runs in polynomial time
\item $\forall x$, $A$ decides if $x \in L$ correctly with probability of at least $\frac{2}{3}$
\end{itemize}


\title{Complete Problems definition} 
Reminder: in general, a language L is said to be "complete" in the class DQC1, if:
\begin{itemize}
\item $L \in DQC1$
\item $\forall L_{0} \in DQC1$ there is a reduction from $L_{0}$ to L, such that the reduction algorithm is in DQC1  
\end{itemize}
Next, we show that calculating an estimate of the trace of a unitary matrix is DQC1-complete


\subsection{trace estimation algorithm  }

The trace estimation problem input is a quantum circuit. Its output should be estimation to the trace of that circuit's trace.
We now show an algorithm that solves this problem in DQC1.


The start state of any DQC1 problem is one clean qubit (state $\ket{0}$), and n-qubits in the maximally mixed state.
That is, the start state is
\begin{equation}
\rho = \ket{0}\bra{0}\otimes\frac{I}{2^n}
\end{equation}
We can use the Hadamard test, which gets as input this state, in order to accurately estimate a trace
of a unitary operation U.


\subsubsection{The Hadamard Test}
The description of the Hadamard test for some unitary matrix U is:
\\\\
\Qcircuit @C=1em @R=.7em {
	\lstick{\ket{0}} & \gate {H} & \ctrl{1} & \gate {H} & \meter & \qw \\
	\lstick{\psi} & {/} \qw & \gate {U} & {/} \qw & \qw & \qw
}
\\\\\\
We will show that this circuit indeed calculates the trace of U
\begin{itemize}
\item after the first Hadamard gate, the state is $\Ket{+}\psi = \frac{1}{\sqrt{2}}\Ket{0}\Ket{\psi} + \frac{1}{\sqrt{2}}\Ket{1}\Ket{\psi}$
\item after the C-U operation, the new state is $\frac{1}{\sqrt{2}}\Ket{0}\Ket{\psi} + \frac{1}{\sqrt{2}}\Ket{1}U\Ket{\psi}$
\item after the final Hadamard operation, the final state becomes $\frac{1}{2}\Ket{0}\Ket{\psi} + \frac{1}{2}\Ket{1}\Ket{\psi}\ +\frac{1}{2}\Ket{0}U\Ket{\psi}\ -  \frac{1}{2}\Ket{1}U\Ket{\psi} = 
\frac{\Ket{\psi} + U\Ket{\psi}}{2}\Ket{0} + \frac{\Ket{\psi} - U\Ket{\psi}}{2}\Ket{1}$
\end{itemize}


Therefore, the probability for measuring 0 at the end of the calculation is:
\begin{align*}
    \rho_{0} &= \sum_{x \in \{0,1\}^n} (\frac{\Bra{x} + \Bra{x}{U^\dagger}}{2})(\frac{\Ket{x} + U\Ket{x}}{2}) = \sum_{x \in \{0,1\}^n}\frac{1}{4}(\Bra{x}\Ket{x} + \Bra{x}{U^\dagger}\Ket{x} + \Bra{x}U\Ket{x} + \Bra{x}{U^\dagger}U\Ket{x}  ) = \\
    &= \sum_{x \in \{0,1\}^n}\frac{1}{2} + \frac{1}{4}(\Bra{x}{U^\dagger}\Ket{x} + \Bra{x}U\Ket{x}) = \sum_{x \in \{0,1\}^n}\frac{1}{2} +  \frac{1}{2}Re(\Bra{x}U\Ket{x})
\end{align*}
And since $\bra{x}U\ket{x}$ is actually $U_{x,x}$
\begin{displaymath}
    \frac{1}{2^{n}}\sum\limits_{x \in \{0,1\}^n} \frac{1+Re(\Bra{x}U\Ket{x})}{2} = \frac{1}{2} + \frac{Re(TrU)}{2^{n+1}}
\end{displaymath}
Therefore, the problem of trace estimation can be solved with one clean qubit easily, by performing the Hadamard test on the input circuit U.

\subsection{Hardness of trace estimation}
\title {The hardness of trace estimation}
Next, we will want to show that trace estimation is hard in DQC1.

That is, we will show that any language in DQC1 can be "transformed" to a trace estimation problem, and that the "transformation" can be done in DQC1.
More formally,  we have some language $L \in DQC1$, and some x, and we want to decide if $x \in L$, by:
\begin{itemize}
\item transform the problem of L and input x, to the trace estimation problem, with some input f(L, x)
\item solve the trace estimation problem.
\end{itemize}

We have to show such a function "f".

Lets start by looking on how general computation in DQC1 looks like.
$L \in DQC1$, therefore its start state obeys equation 1. on this start state, we apply some unitary matrix U(x) (U must be function of the input,since the input to U is always the same in the DQC1 problems). and get the state $\rho_{final} = U\rho{U^\dagger} = U\ket{0}\bra{0}\frac{I}{2^n}{U^\dagger}$
Therefore, the probability to measure 0 at the end of the computation (recall, the probability to measure 0 tells us if $x \in L$ or not), is equal to the trace of the submatrix of the final matrix in which the first qubit is in state $\ket{0}$:
\begin{equation}
 p_{0} = Tr[\Ket{0}\Bra{0}\otimes I\rho_{final}] = 2^{-n}Tr[(\Ket{0}\Bra{0}\otimes I)U(\Ket{0}\Bra{0}\otimes I{U^\dagger})]
\end{equation}

At first glance, it seems that we can just perform trace estimation to the  matrix $(\Ket{0}\Bra{0}\otimes I)U(\Ket{0}\Bra{0}\otimes I{U^\dagger})$, and be done with it.

Unfortunately - this matrix isn't unitary, so our trace estimation algoritm won't work!!

%% proof - $AU(AU)^\dagger=AUU^\daggerA^\dagger=AA^\dagger \ne I


Given some circuit M, we examine the following quantom circuit C(M):
\\
\Qcircuit @C=1em @R=.7em {
	& \qw & \multigate{1}{M^\dag} & \ctrl{2} & \multigate{1}{M} & \ctrl{3} & \qw \\
	& {/} \qw & \ghost{U^\dag} & \qw & \ghost{U} & \qw &  {/} \qw \\
	&  \qw &\qw &  \targ  & \qw & \qw & \qw \\
	&  \qw & \qw & \qw & \qw & \targ & \qw
}
\\
Proposition 1.1: $\frac{1}{4}$tr[C]=$Tr[(\Ket{0}\Bra{0}\otimes I)M(\Ket{0}\Bra{0}\otimes I{M^\dagger})]$

Proof:
\begin{itemize}
\item first, lets remember that $tr[C] = \sum\limits_{\psi \in {0,1}^n} \Bra{\psi}C\Ket{\psi}$
and in a similar way, $Tr[(\Ket{0}\Bra{0}\otimes I)M(\Ket{0}\Bra{0}\otimes I{M^\dagger})] = \sum\limits_{\psi \in {0,1}^n} \Bra{\psi}(\Ket{0}\Bra{0}\otimes I)M(\Ket{0}\Bra{0}\otimes I{M^\dagger})\Ket{\psi}$
\end{itemize}

\begin{itemize}
\item that is, some state $\psi$ contribute to the trace of $(\Ket{0}\Bra{0}\otimes I)M(\Ket{0}\Bra{0}\otimes I{M^\dagger})$ iff after imply $M^\dagger$ it has some
component with zero in its first qubit, and then after implying M on that component, we still remain with some non-zero component in the first qubit
\item Similarly, in C, if, say, the result of implying $M^\dagger$ on $\psi$ give a non-zero component, the one of the two last qubits will "flip", creating a state orthogonal to the original state
(and similarly on M).
\item therefore, the two circuits traces has the exact same components and are equal, up to factor of 4
(which come from the "free choice" in the values of the two last qubits in C)
 
\end{itemize}

Notice that if M is unitary then C(M) is unitary, so in order to estimate $2^{-n}Tr[(\Ket{0}\Bra{0}\otimes I)U(\Ket{0}\Bra{0}\otimes I{U^\dagger})]$, we can build C(U) and
estimate its trace, thus getting approximation to  $2^{-n}Tr[(\Ket{0}\Bra{0}\otimes I)U(\Ket{0}\Bra{0}\otimes I{U^\dagger})]$, and from that estimation
we know whats the probability that the computation of U(x) will accept, i.e. we know if $x \in L$ or not.



\subsection{adding few more clean bits don't give extra power}
\title{adding few more clean bits don't give extra power}
We showed that computing the matrix trace is DQC1-complete problem.
We notice now that actually we didn't computed it accurately. Since only the expectation of the algorithm is the trace, we rather get an approximation. According to the Chernoff inequality (which says: \(Pr[X > np +x] \leq e^{\frac{-x^{2}}{2np(1-p)}}\)), the probability of being wrong with more then \(\frac{1}{n}\), is at most \(O(e^{-n})\), so we can assume (with probability of almost 1), that we got a polynomial approximation to the expression $\frac{Tr[U]}{2^{n}}$,  or  in other words we have a $\frac{2^{n}}{poly(n)}$ additive- approximation to the trace. Notice that such approximization is enough for deciding any other language in DQC1, by the algorithm described in section 1.3.

We will define now the DQCK complexity class (Informmaly, DQCK is identical to DQC1, except we are allowed to start with K clean qubits) :
DQCK class is the class of decidable languages with algorithm A such that:
\begin{itemize}
\item A runs in polynomial time
\item $\forall x$, A decide correctly if $x \in L$, with probability of at least $\frac{2}{3}$. 
In case that $x \in L$, we expect the first clean qubit to be 0 at the end of the algorithm
\item A start state includes K clean qubits (K can be a constant, or a function of n) (state $\ket{0}$), and n-qubits in the maximally mixed state
\item A may perform any unitary operation on his start state
\item A can measure only the clean qubits, only at the end of the algorithm
\item We assume that A is invoked cant be invoked many times in parallel
\item A has no access to classical computer, so its not promised that $P \subset DQCK$ 
\end{itemize}  

We will now prove that for $k \leq \log{n}$, estimate the trace of unitary matrix with the same precision is still a complete problem, thus proving that adding logarithmic number of clean bits doesn't change the power. 

Obviously we can calculate the trace of unitary matrix with $\log{n}$ bits, since we can do it just with one. 


As for the less trivial direction, assume we have some quantum algorithm in DQCK.
Similarly to the one-qubit option, final state is:
$\rho_{final} = U\rho{U^\dagger} = U{\ket{0}\bra{0}}^{\otimes k}\frac{I}{2^n}{U^\dagger}$
and therefore the probability of measuring 0 at the end is:
$p_{0} = Tr[{\Ket{0}\Bra{0}} \otimes I\rho_{final}] = 2^{-n}Tr[(\Ket{0}\Bra{0}\otimes I)U({\Ket{0}\Bra{0}}^{\otimes k}\otimes I{U^\dagger})$

Now, we have the same problem at estimating this matrix: its not unitary!
To resolve that issue, we build circuit similar to the one in the 1-clean qubit process,
but now we add k ancilla qubits, when there is a CNOT gate between each i-th qubit and (i+n)-th qubit (for the first k qubits).
\\
\Qcircuit @C=1em @R=.7em{
  \qw & \qw & \multigate{2}{U^\dag} & \qw & \ctrl{3} & \multigate{2}{U} & \ctrl{4} &  \qw & \qw \\
  \qw & {/} \qw & \ghost{U^\dag} & \ctrl{2} & \qw & \ghost{U} & \qw & \qw & \qw\\
  \qw & {/} \qw & \ghost{U^\dag} & \qw &  \qw &\ghost{U} & \qw & {/} \qw & \qw \\
  \qw & {/} \qw & \qw & \targ  &\targ & \qw & \qw & {/} \qw & \qw \\
  \qw & \qw & \qw & \qw & \qw & \qw & \targ & \qw & \qw \\
}
\\
(The first line contains one clean qubit. The second line include the other $k-1$ clean qubits. The third line contains the n maximmaly-mixed qubits.
The forth line contains k ancilla qubits. The fifth line contains another ancilla qubit.)

Now, we can see (similarly to the proposition 1.1), that the trace of this new circuit $U^{*}$ follows the rule: $Tr[U^{*}] = 2^{k}Tr[(\Ket{0}\Bra{0}\otimes I)U({\Ket{0}\Bra{0}}^{\otimes k}\otimes I{U^\dagger}$. Thus, in polynomial number of executions we can compute its trace up to a precision of $\frac{2^{n+k}}{poly(n,k)}$, but this equals to $\frac{2^{n}}{poly(n)}$ when $k  \leq \log{n}$, which means that in this case the precision is good enough to decide the original problem.


\section{Mathematical background for the Jones Polynomial understanding}
\subsection{Knots}
In the rest of this paper, we will define the Jones Polynomial, and try to figure out how to approximate it in one point, by a DQC1 - algorithm.
First, we have to fully understand what a Jones polynomial is. This section will "build up" The definition of the Jones polynomial.

A knot is a closed, non-self-intersecting curve that is embedded in three dimensions and cannot be untangled to produce a simple loop (if it can, it will be the unknot). Figure 1 and 2 gives some examples ofd knots.

\begin{figure}
\centering
\includegraphics[scale=0.25]{470px-Knot_table}
\caption{Single strand knots}
\label{fig:my_label}
\end{figure}

\begin{figure}
\includegraphics[scale=0.25]{307px-Hopf_link} 
\caption{Hopf link}
\end{figure}

A basic question that arise when someone exmines several knots, is how to know whether they are topology equivakent or not. Even more basic question: how to differ between the unknot and the other knots?

Reidemeister showed in 1927 the "Reidemeister moves", that do not change the knot.
\begin{figure}
\includegraphics[scale=1]{Reidemeister} 
\caption{Reidemeister moves}
\end{figure}

However it is still not known how to construct a swquence of such moves between two random equivakent knotes, or peove that there isn't such sequence.

\subsection{The Kaufmann Polynomial}
The Jones Polynomial comes for the rescue here: We will compute a jones polynimial for every knot. By definition of the Jones polynomial, equivalent knots will have the same Jones polynomial (to be proven later). However, as we will see, compute the Jones polynomial is itself a very hard task.

  We will first define the Kaufmann Bracket Polynomial, which is "almost" correct.

Consider a knot K. for each crossing in K, from the form 
\begin{center}
\begin{tikzpicture}
\braid[rotate=0, height=.3cm, style strands={1}{ red } ,style
strands={2}{ blue } ] s_1;
\end{tikzpicture}
\end{center} 

We decide at random to replace it with one of two options:
\begin{itemize}
\item One of the strands goes to the right, the other one remains on the left (considered choice 1 in the case that the upper-left strand was originaly "obove" the other one, like in the picture above. considered choise "2" in case that the "upper-right" strand was originally above the other strand (i.e. if the blue strand was above the red one)
\item One of the strands remains on the top, the other strand remains at the bottom (considered choice 2 in the case that the left strand was originaly "obove" the right one,  like in the picture above considered choise "1" in case that the "right" strand was originally above the left strand(i.e. if the blue strand was above the red one))
\end{itemize}
Each decision like this for all the crossings is a state \(\sigma\)

\begin{figure}
\includegraphics[scale=0.2]{kauffman_calc} 
\caption{kaufmann bracket example}
\end{figure}

We denote by \(\sigma_{+}\) the number of replaces we made choice 2, and by 
\(\sigma_{-}\) the number of replaces we made choice 1.
We denote by\(N_{\sigma}\) the number of loops created when all the changes of \(\sigma\) are applied.
The Kaufmann Bracket Polynomial is defined as:
\( L(A) = \sum\limits_{all_states_\sigma}{A^{\sigma_{+} -\sigma_{-}}d^{N_{\sigma} - 1}}\)

when:
\begin{itemize}
\item L stands for the Kaufmann polynomial.
\item A is a point that we want to evluate this polynomial at.
\item d is defined as \(d = -A^{-2} -A^2\) 
\end{itemize}

Lets see some examples of this definition:
\begin{itemize}
\item \(\forall A\), L(A)=1 in the unknot:
    \begin{itemize}
    \item we have only one state $\sigma$, with \(\sigma_{+}=0,\sigma_{-}=0\)
    \item this state have one loop, so \(N_{\sigma} = 1\)
    \item Therefore, $L(A) = A^{0}d^{0}=1$
    \end{itemize}
\item For two unknots:
    \begin{itemize}
    \item we have only one state $\sigma$, with \(\sigma_{+}=0,\sigma_{-}=0\)
    \item this state have two loops, so \(N_{\sigma} = 2\)
    \item Therefore, $L(A) = A^{0}d^{1}=d=-A^{-2}-A^{2}$
    \end{itemize}
\end{itemize}

\begin{figure}
\includegraphics[scale=0.17]{kauffman_calc}
\caption{kaufmann polynomial, third example}
\end{figure}

Lets now exmine a bit more complicated case, as shown in Figure 5:
\begin{itemize}
    \item we have two states. In choice 1, $\sigma_{-}=1$, In choice 2,$\sigma_{+}=1$
    \item If we chooce choice 1 we have \(N_{\sigma} = 1\), if we choose choice 2 we have \(N_{\sigma} = 2\)
    \item Therefore, $L(A) = A^{-1}d^{0} + A^{1}d^{1} = A^{-1} +A^{1}(-A^{-2}-A^{2}) = -A^{3}$
\end{itemize}

\begin{figure}
\includegraphics[scale=0.2]{kauffman_bracket_identity} 
\caption{kaufmann polynomial, recursive formula}
\end{figure}

\subsection{The Kaufman polynomial recursive formula}

The recursive formula for calculating the Kaufman polynomial is described at Figure 6.
The formula comes directly from the definition, if we consider only one cross. We can use it to simplify the calculation of some more complicated knots (no need to track explicitly  $\sigma_{-}, \sigma_{+}, N_{\sigma}$).

Note, However, that this formula means that the number of elements in the polynom is exponential in the number of crosses.

A slightly more complicated computation of Kaufman polynomial, using this formula, is shown in Figure 7.
\begin{figure}
\includegraphics[scale=0.2]{hopf_link} 
\caption{the kaufmann polynomial of the Hopf link}
\end{figure}


\subsection{The Jones Polynomial}
Remember that factor of $A^{-3}$ we got in one of the previous examples, although it was the same as the unknot?

Jones polynomial will solve this problem, by adding a relevant normalization factor.

We define by the w(k) for a knot k to be: \(w(k) = \sum\limits_{all crossings}{(-1)^{is the left arrow above the right one}}\)
and Jones polynomial is defines as: \(V_{k}(t)=V_{k}(A^{-4})=(-A)^{3w(k)}L_{k}(A)\).

Notice that w(k) for the unknot is 0, so the Jones polynomial of the unknot is still equals to 1 at any point.

Consider the Hopf link.

Moving to Jones Polynomial, we can see that w(HopfLink)=-2 (two cross with the same oreintation), so:
\(V_{hopfLink}=(-A)^{-6}(-A^{-4}-A^{4}) = -A^{-2} - A^{-10}\).
Remember that \(t = A^{-4}\) and we get \(V_{hopfLink}(t)=-\sqrt{t}(1+t^{2})\)  

Now, we have to show that Jones polynomial will be the same for 2 equivalent Knots, thus proving the property that caused us to define it in the first place.One way to do it is to prove that ist doesn't change under the Reidemeister moves.

Note that we already saw that Jones polynomial will be the same for the first Reidemeister move (At the computation of the Kaufmann polynomial for the example in Fig.5, which is the same as the Unknot up to the first Reidmister move, we got $A^{3}$ When combined with the Jones Polynimal normalization, we get $V(Fig-5)=A^{3}A^{-3} = 1$.

In Figure 8 and 9, you can see a computation for the second and third moves, correspondinegly.

\begin{figure}
\includegraphics[scale=0.5]{second_rule} 
\caption{The jones invarability under the second move}
\end{figure}

\begin{figure}
\includegraphics[scale=0.5]{thirdRule} 
\caption{The jones invarability under the third move}
\end{figure}


\section{Preliminary algebra knowledge}

\subsection{Braid groups}
We said that Jones Polynomials are defined over knots. However, how are we going to represent a knot computationally?
Eventually, the knots will be represented by a finite sequence of bits... 
We have to create some objects that will "capture" all the possibilities of a knot, but will be easily represented.

For that, we have the braid groups.
\begin{definition}
Consider two horizontal bars, one on top of the other, with n-points each. A n-stand braid, is
n strands, such that:
\begin{itemize}
\item each strand has exactly one peg attached to it on each bar
\item the strands may cross one over another
\item at every point, the strand direction has non-zero component directed down 
\end{itemize}
\end{definition}

We call to the collection of all such braids the braid group $B_{n}$, with identity element and generators that will immediately follow, when the operation between two brands is put them one below the other, and connect all the button-pegs of the first, with the top-pegs of the second.

\title{examples}
The identity braid:
\begin{center}
\begin{tikzpicture}
\braid[rotate=0,number of strands = 3, style strands={1}{ red } ,style
strands={2}{ blue } ,style strands={3}{ green } ]; 
\end{tikzpicture}
\end{center}

Some other braid:
\begin{center}
\begin{tikzpicture}
\braid[rotate=0, style strands={1}{ red } ,style
strands={2}{ blue } ,style strands={3}{ green } ] s_1 s_1^{-1} 
s_2;
\end{tikzpicture}
\end{center}
 


\title{braid group generators}
$\forall 1\leq i \leq n$, we denote by $\sigma_{i}$, the braid which takes the i-th strand
to the (i+1) place, the (i+1)-strand to the i-th place, and leaves all the other strands in place.(the (i)th strand will be above in the collision area) 
For example, this is $\sigma_{2}$ with 3-strands:
\begin{center}
\begin{tikzpicture}
\braid[rotate=0, style strands={1}{ red } ,style
strands={2}{ blue } ,style strands={3}{ green } ]s_2;
\end{tikzpicture}
\end{center}

Notice that when $1 < |i - j|$ then $\sigma_{i}, \sigma_{j}$ act on completely different strands, so they are commutative: $\sigma_{i}\sigma_{j} = \sigma_{j}\sigma_{i}$

Furthermore, it holds that $\sigma_{i}\sigma_{i+1}\sigma_{i} = \sigma_{i+1}\sigma_{i}\sigma_{i+1}$
(both switch the i-th strand and the (i+2)-strand, while leave all the others in place)


\title{braid groups and knots}
Braids can create a knot by "joining the loose ends" all together. there are two ways of doing it:
The plat closure, where we join neighbour pegs in the top and bottom, and the trace closure, where we connect each top peg to the corresponding bottom peg, without creating more loops.
for example, for the following braid: 

\begin{center}
\begin{tikzpicture}
\braid[rotate=0, height=.3cm, style strands={1}{ red } ,style
strands={2}{ blue } ,style strands={3}{ green } ,style strands={4}{ black } ] s_1 s_2^{-1} 
s_3 s_1 s_2^{-1} s_1^{-1} s_2;
\end{tikzpicture}
\end{center} 
you can see the different clousers on figure 10.
\begin{figure}
\includegraphics[scale=0.5]{closures} 
\caption{plat and trace closures}
\end{figure}

For each braid group generator, we define its inverse element to be the element which takes the i-th strand
to the (i+1) place, the (i+1)-strand to the i-th place, and leaves all the other strands in place. (the (i+1)th strand will be above in the collision area).

For example this is the $\sigma_{2}^{-1}$ with 3 strands:
\begin{center}
\begin{tikzpicture}
\braid[rotate=0, style strands={1}{ red } ,style
strands={2}{ blue } ,style strands={3}{ green } ]s_2^{-1};
\end{tikzpicture}
\end{center}

Notice that concatenation of a generator and its inverse really bring us back to the identity element:
\begin{center}
\begin{tikzpicture}
\braid[rotate=0, style strands={1}{ red } ,style
strands={2}{ blue } ,style strands={3}{ green } ]s_2 s_2^{-1};
\end{tikzpicture}
\end{center}

We can simply "pull left" the blue strand, and return to the identity element.

Last, we still need to proof that the Braid group is indeed a group. This is trivially follows from the observasion that any braid group element can be written as a multiplication of the braid group generators and thier inverses (simply follow the crosses between the braids, and write the corresponding generators). Therefore:
\begin{itemize}
\item The identity element exist, and its simply the element which incluse the "identity permutation"
\item since, as we saw, each generator has an inverse, and each element can be writtem as multiplication of generators, it meanse that every element has an inverse.
\item The operation on 2 braid group elements yields of course another braid group element.
\end{itemize}

\subsection{The temperly-Lieb algebra}

Our long term goal is to connect somehow the Jones polynomial value to a function that we can calculate easily.
But... function of what? 
A nice idea will be to define instances of strand groups that are able to create circles, and behave like the Jones polynomial.

First, lets see a formal definition of the temperly-Lieb algebra:
Temperley Lieb[n,d] algebra is ab algebra over the field of polynomials with coeffients in Z, such that:

There exists generators $E_{1}....E_{n-1}$, that obeys the following rules:
\begin{itemize}
\item \(E_{i}E_{j} = E_{j}E_{i}\), when \(2 \leq |i-j|\)
\item \(E_{i}E_{i+1}E_{i} = E_{i}\), \(E_{i}E_{i-1}E_{i} = E_{i}\)
\item \({E_{i}}^2 = dE_{i}\)
\end{itemize}

Each Temperley-Lieb element can be obtained as multiplication of these generators.

We won't treat these generators as polynoms. Rather, we will work with "graphic visualization" of that algebra. Graphicallt, the temperely-Lieb algebra defined as follows:
Tempely Lieb {n,d} algebra group consists of two rows of pegs (button and top), and n-strands, such that:
\begin{itemize}
\item each strand connects to exactly two pegs (but they can be from the same side!)
\item the strands cannot intersect between them.
\end{itemize}

In Figure 11 you can see an example to some Temerley-Lieb algebra objects
\begin{figure}
\includegraphics[scale=0.25]{tempely_lieb_generators} 
\caption{tempely lieb objects example}\
\end{figure}

Under this definition, the i-th Temperley-Lieb algebra generator will be the object which sends each bottom/top peg to is equivalent in the other row, except the i-th and the (i+1)th pegs : the top ith peg will go to the (i+1)th-top peg, and the ith bottom peg will go to the (i+1)th bottom peg. Notice that an example to generators can also be seen in Figure 10: the first picture there is of $E_{2}$ - the second generator.

The operation on 2 Templey-Lieb object is a simple contanenation. in case that a circle is created, it is removed, and we multiply the object by d (Recall that all the objects are actually polinomials, and this is only graphic view of them, so multiply by d is a well-defined action).

Figures 12-14 show examples to the action between templerley-Lieb objects, and also proves that this graphic representation obeys the 3 rules about the Temperley-Lieb generators.
\begin{figure}
\includegraphics[scale=0.5]{tempely_lieb_first_rule} 
\caption{$E_{1}E_{3}=E_{3}E_{1}$}\
\end{figure}

\begin{figure}
\includegraphics[scale=0.5]{tempely_lieb_second_rule} 
\caption{$E_{1}E_{2}E_{1}=E_{1}$}\
\end{figure}

\begin{figure}
\includegraphics[scale=0.5]{tempely_lieb_third_rule} 
\caption{$E_{1}E_{1}=dE_{1}$}\
\end{figure}

\subsection{From Braid group to Temperley Lieb algebra}
We know by now, that our knot is going to be represented as a braid-group object, yet the action on it $Tr$ that will yield the Jones Polynomial value will get objects from the Tmperley-Lieb world. Thus, we need some "translation" from a braid group element to a temperley-Lien element. 

We will define the following homomorphism
$\rho_{A}(\sigma_{i}) = AE_{i} +A^{-1}I$, when I is the identity in the Temperley-Lieb algebra,
and A is a number that satisfies $A^{2}+A^{-2}=d$.
We can show that this is indeed a representation of the braid groups as temperly-Lieb objects, if we show that the relations
of the braid group generators still hold.
As for the first relation, it holds that for $2 \leq |i-j|$ , $\rho_{A}(\sigma_{i}),\rho_{A}(\sigma_{j})$ commutes since in this case $E_{i},E_{j}$ commutes as well.

\title{From braid groups to Temperley-Lieb algebra (2)}
For the second relation, to show that :
\begin{displaymath}
\rho_{A}(\sigma_{i})\rho_{A}(\sigma_{i+1})\rho_{A}(\sigma_{i})= \rho_{A}(\sigma_{i+1})\rho_{A}(\sigma_{i})\rho_{A}(\sigma_{i+1})
\end{displaymath}
or:

$
A^{3}E_{i}E_{i+1}E_{i} +AE_{i}E_{i+1} + AE_{i}^{2} + A^{-1}E_{i} +AE_{i+1}E_{i}+A^{-1}E_{i+1} + A^{-1}E_{i} + A^{-3}$

=

$
A^{3}E_{i+1}E_{i}E_{i+1} +AE_{i+1}E_{i} + AE_{i+1}^{2} + A^{-1}E_{i+1} +AE_{i}E_{i+1}+A^{-1}E_{i} + A^{-1}E_{i+1} + A^{-3}
$
after delete similar elements and use some generators identities, we have to show that:
\begin{displaymath}
(A^{-1}+Ad+A^{3})E_{i}= (A^{-1}+Ad+A^{3})E_{i+1}
\end{displaymath}
and this is correct since $(A^{-1}+Ad+A^{3}) = 0$

\subsection{The Markov Trace}
Its finally the time to define the function Tr, the function that acts on Temperley-Lieb elements, and will be connected to the Jones polynomial value.
The Markov Trace is a function on a Temperley-Lieb algebra, which defined as follows:
\begin{itemize}
\item given a Temperley-Lieb algebra object, we connect its bottom and top bars, in a similar way to
a trace closure.
\item when denote the number of loops created like this with a, the trace closure is $d^{a-n}$
\end{itemize}.

Figure 15 shows a calculation example for this function on $E_{1}$, at Templey-Lieb(3,d) algebra.

\begin{figure}
\centering
\includegraphics{MarkovTraceExample}
\caption{example of calculation of the markov trace function}
\label{fig:my_label}
\end{figure}

\newtheorem{theorem}{Theorem}
\begin{theorem}{
The Markov trace Tr obeys the following:
\begin{itemize}
\item Tr[1] = 1 (the identity Temperley-Lieb algebra has n loops in its closure, $d^{n-n} = 1$)
\item $\forall X,Y \in TL[n,d]$, Tr[XY] = Tr[YX] 
\item $\forall X \in TL[n-1, d], Tr[xE_{n-1}]=\frac{Tr[x]}{d}$ (add $E_{n-1}$ add new peg but don't enlarge the number of loops).
\end{itemize}
\end{theorem}

\begin{proof}
The first part can be proven trivially by instantiate the function.
As for the second part, notice that the Markov trace relate only to closed loops, and doesn't care whether we look on the Element "bottom-up" or "top-down" - the number of loops will remain the same. Therefore, Markov trace is commutative.
As for the last step, notice that in case of addition of $E_{n-1}$, we enlarge the number of strands by 1, but the number of loops didn't change, as the last strand will "join" to the existing loop of the strand before him. When taking this into account and look in the Markov trace definiton, the result immediatly follows.
\end{proof}


We will now turn to prove a less trivial claim: 
\begin{theorem}
The Markov Trace function is the only function of Templey-Lieb objects, that obeys rules 1-3. That is, if some unknown function obeys these rules, then it must be the Markov Trace.
\end{theorem}

\begin{proof}
We will regard each object as a word with letter from the the group $E_{1}...E_{n}$
We will define such word to be "reduced", if it wont be equal to any "shorter" word.
The proof is done in induction on n, the maximal generator index in the reducible word:
\begin{itemize}
\item As for the base case, there is only one object with maximal generator 0 - the identity object, and its value is set to 1.
\item assume that the value is well defined for any reducible object maximal generator $\leq n$ and lets prove it for n+1. 
\end{itemize}


\title{The uniqueness of the Markov Trace (2)}
First, we will prove that in any such reducible word contains exactly one $E_{n}$ generator.
This will suffice, since then we can write $w=w_{1}E_{n}w_{2}$, when $w_{1}, w_{2}$ are
reducible words with maximal generator n-1, which their value is well defined by the induction assumption.

Then, it holds that $Tr[w]=Tr[w_{1}E_{n}w_{2}] = Tr[w_{1}w_{2}E_{n}] = dTr[w_{1}w_{2}]$, and
since $w_{1}w_{2}$ is another reducible word that its value is well defined, the proof will be complete.   


\title{The uniqueness of the Markov Trace (3)}
Assume the contrary, that is there is a reducible word that contains two $E_{n}$.
Then, we can write $w=w_{1}E_{n}w_{2}E_{n}w_{3}$, when $w_{i}$ are reducible with maximal generator
$E_{n-1}$. Now, if $E_{n-1}$ isn't in $w_{2}$, we can change the order (from generators rules:)
$w=w_{1}w_{2}E_{n}E_{n}w_{3}\ = dw_{1}w_{2}E_{n}w_{3}$,so w isn't reducible.
If there is $E_{n-1}$ in  $w_{2}$, we can write:
\begin{displaymath}
 w=w_{1}E_{n}v_{1}E_{n-1}v_{2}E_{n}w_{3},
 \end{displaymath} 
 when 
 $v_{i}$  are reducible with maximal generator
$(E_{n-2}$. Therefore, they can commute with $E_{n-1}$, creating 
\begin{displaymath}
w=w_{1}v_{1}E_{n}E_{n-1}E_{n}v_{2}w_{3}
= w_{1}v_{1}E_{n}v_{2}w_{3}
 \end{displaymath} 
 , so w again is not reducible
   
\end{proof}






\subsection{the connection between Jones Polynomial to Markov trace and braid groups}
We will now show a connection between the Jones Polynomial and the Markov trace.

\begin{theorem}
Let B be some braid group element. Let $B^{tr}$ denote the knot we get from B by the trace closure method. Recall the additional, already defined, notations:
\begin{itemize}
\item V - The Jones polynomial.
\item $\rho_{A}{B}$ = the Templey-Lieb algebra we get by apply the homomorphism $\rho_{A}$ on the braid B.
\item w - the Jones polynomial normaization factor
\item $d = -A^{2}-A^{-2}$
\item Tr- The Markov Trace function
\end{itemize}

Then it holds that
$V_{B^{tr}}(A) = (-A)^{3w(B^{tr})}d^{n-1}Tr[\rho_{A}(B)]$
That is, the Jones polynomial of a trace-closure of some braid is connected to the Markov trace value of its corresponding Temperley-Lieb object.
\end{theorem}

\begin{proof}
We only have to proof that $L(B^{tr}) = d^{n-1}Tr[\rho_{A}(B)]$. Recall that for each crossing in the braid B, the Kauffman polynomial hold the formula described at Figure 16.
\begin{figure}
\includegraphics[scale=0.2]{kauffman_bracket_identity} 
\caption{Kauffman bracket recursive nature}
\end{figure}.
The homomorphism also has the form of  $\rho_{A}(\sigma_{i}) = AE_{i} +A^{-1}I$ - exactly the same!.

\title{the connection to Markov trace and braid groups(2) }
After choosing a state $\sigma$, its value in the Kauffman polynomial is $d^{N_{\sigma} -1}$, and its
value i the Markov trace is $d^{N_{\sigma}} -n$, therefore we need the $d^{n-1}$ factor. 
 \end{proof}

\section{The Fibonacci representation}
We already know that braids are an easy way to represent a knot. But eventually we will have to come up with a quantom algorithm that works on matrices.

Such matrix representation must have some properties that will make it useful to us:
\begin{itemize}
\item for every braid, its matrix must be unitary, so we can link it to some quantom circuit.
\item for every braid, there has to be some connection between the Jones Polynomial of the braid, and the relevant matrix.
\end{itemize}

 So, how we represent a braid as a matrix?
 
As a first intuition, we would expect that the representation of the identity braid will be the identity matrix. This intuition, although seems simple and clear, gives us a clue about our representation method: We will match each crossing in the braid some "little matrix", and the general representation will be the multiplication of all these matrices.

Next, lets attend to a question with less intuitive answer: what is the dimension of this matrix should be? clearly, the dimension should grow with the braid size, however the naive approach of multiply the matrix dimension for each new braid, sounds like a waste.

 Given an n-strand braid, we can write a string of n+1 elements on the bottom of the braid between every two strands, where each element is either * or p. The only restriction is that there will be no two adjacent * elements. The number of possibilities to do so is of course $f_{n+3}$, where $f_{n}$ denotes the $n$th Fibonacci number.

Next, for each crossing and labelling of it (the 3 elements from the right, left, and in the crossing) , we would like to give a linear function (i.e. - a row in the matrix) that will "represent" the crossing, and that may change the center label. (we wouldn't want to change the right or left label, in order to preserve the string of elements correctness under all the operations).

That is, in the most general form, a cross $\sigma_{i}$ with labeling of P*P, can move to 
a times * Identity with labelling P*P + b times the identity with labelling PPP.
we will denote it by $(p\hat{*}p)=a(p*p)+b(ppp)$

Lets take a look on the explicit, formal representation, and them see some examples:

$(*\hat{p}p)=a(*pp)$

$(*\hat{p}*)=b(*p*)$

$(p\hat{*}p)=c(p*p)+d(ppp)$

$(p\hat{p}*)=a(pp*)$

$(p\hat{p}p)=d(p*p)+e(ppp)$

, with:
$ a = -A^{4} $


 $  b = A^{8}  $
 
 $  c = A^{8}\tau^{2} - A^{4}\tau $
  
 $  d = A^{8}\tau^{\frac{3}{2}} + A^{4}\tau^{\frac{3}{2}} $ 
 
 $  e = A^{8}\tau - A^{4}\tau^{2} $ 
 
 $  A = e^{\frac{-3{\pi}i}{5}} $ 
 
 $  \tau = \frac{2}{1 + \sqrt{5}} $


 
\title{The Fibonacci representation - examples}
First, notice that indeed, the representation of the identity braid, is the identity matrix. Next, lets show the representation of $\sigma_{1}$, with 2 strands. The dimension of the matrix should be $f_{2+3}=f_{5}=5$. The matix itself will be:

\begin{pmatrix}
b & 0 & 0 & 0 & 0 \\ 0 & a & 0 & 0 & 0 \\ 0 & 0 & a & 0 & 0 \\ 0 & 0 & 0 & c & d \\ 0 & 0 & 0 & d & e 
\end{pmatrix}
\begin{pmatrix} *p* \\ *pp \\ pp*\\ p*p \\ ppp
\end{pmatrix}

Next, We will show the Fibonacci representation of $\sigma_{1}$ with 3 strands:
\[
\begin{pmatrix} b & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & a & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & a & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & e & 0 & 0 & 0 & d \\ 0 & 0 & 0 & 0 & a & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & e & d & 0 \\0 & 0 & 0 & 0 & 0 & d & c & 0 \\0 & 0 & 0 & d & 0 & 0 & 0 & c \end{pmatrix} 
  \begin{pmatrix} *p*p \\ *ppp \\ *pp* \\ pppp \\ pp*p \\ ppp* \\ p*p* \\ p*pp \end{pmatrix}
\]


Now, We will show the Fibonacci reoresentation of $\sigma_{2}$ with 3 strands:
\[
\begin{pmatrix} c & d & 0 & 0 & 0 & 0 & 0 & 0 \\ d & e & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & a & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & e & d & 0 & 0 & 0 \\ 0 & 0 & 0 & d & c & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & a & 0 & 0 \\0 & 0 & 0 & 0 & 0 & 0 & b & 0 \\0 & 0 & 0 & 0 & 0 & 0 & 0 & a \end{pmatrix} 
  \begin{pmatrix} *p*p \\ *ppp \\ *pp* \\ pppp \\ pp*p \\ ppp* \\ p*p* \\ p*pp \end{pmatrix}
\]

And what about the representation of $\sigma_{1}\sigma_{2}$ with 3 strands? In that case , we will just multiply the matrices from the both previous examples.

\subsection{The Fibonacci representation is indeed a representation}
In the following section we will prove the following:
\begin{theorem}
The Fibonacci representation is indeed a representation. 
\end{theorem}

\begin{proof}
In order to show that this is a representation, we will have to show that the braid group generators properties are preserved under this new representation. As we saw, every line in the generators representation includes few non-zero entries. We will use that into our advantage.

The first property we have to proof is that $\rho_{f}(\sigma_{i})\rho_{f}(\sigma_{j}) = \rho_{f}(\sigma_{j})\rho_{f}(\sigma_{i})$, when $2 \leq |i - j| $. Lets look on some cell at the matrix $\rho_{f}(\sigma_{i})\rho_{f}(\sigma_{j})$. Lets say, that it comes from the multiplication of line $n$ in $\rho_{f}(\sigma_{i})$ with coloum $m$ at $\rho_{f}(\sigma_{j})$. 
Most of the entries in the multiplication will obviously be "0". When they will not be 0?

First, notice that in the multiplication of line $n$ in $\rho_{f}(\sigma_{i})$ with coloum $m$ at $\rho_{f}(\sigma_{j})$, there will be at most 1 non-zero term. That is because each line contains at most 2 non-zero cells. If one of the lines contains only 1 non-zero cell, we finished. Else, that means that each line contain non zero term in the diagonal and in the index that corresponds to a "single substitution". But, since the substitution are happened in different places (for the different $\sigma$'s), we will have at most one "non-zero collision"

Lets now see that is that non-zero term from the perspective of $\rho_{f}(\sigma_{i})$

\begin{itemize}
\item if it the term that comes from the term at the diagonal of $\rho_{f}(\sigma_{i})$, That means that either m=n,or m "is close to n up to one change of p/*". 
\begin{itemize}
\item if m=n, then (since the matrices are symmetric), we will get the exact same result when we will multiply the nth row of  $\rho_{f}(\sigma_{j})$ with the mth colum of $\rho_{f}(\sigma_{i})$
\item if m "is close to n up to one change of p/*", then (since $2 \leq |i - j| $), it means that from the perspective of $\rho_{f}(\sigma_{j})$, m and n are "the same" (i.e. they both contain the same 3 symbols in the place of $\sigma_{i}$, and so the m-th and n-th rows of $\rho_{f}(\sigma_{j})$  will be exactly the same! That yields (since m!=n), that in any case we will get the single value on the m-th and n-th rows
\end{itemize}
\item if it the term that comes from a non-diagonal term, then either it comes from the diagonal of $\rho_{f}(\sigma_{j})$ (w.l.o.g. we handled that case already), or it comes from a non-diagonal place at both matrices. That means that there is a "2 places diff" between m and n, and the result must be $d^{2}$. when we will look on the multiplication of the nth row of $\rho_{f}(\sigma_{j})$ with the mth coloumn of $\rho_{f}(\sigma_{i})$, it still hold that there is a "2 places diff" between  and n, and in addition, we now that both m and n havinf the for of "ppp /p*p", from the perspective of $\rho_{f}(\sigma_{j})$,  $\rho_{f}(\sigma_{i})$, leads us to the conclusion that this multiplication result will be $d^{2}$ as well.
\end{itemize}

\end{proof}

\subsection{Unitarity}
\title{The Unitary of the Fibonacci representation}
Its enough to prove that all the braid group generators, since multiplication of unitary matrices is again unitary matrix. Each generator like this include only one cross. As we saw in the definition and examples, each cross leaves only 1-2 non-zero elements in each matrix row, in the corresponding labelling entry.

we will move onto the 5 options to label the 3 elements near the cross (since the other elements doesn't matter here).
\begin{itemize}
\item with all the labelling with the form ...*pp... , the unitary row will include only one "a" in the matrix diagonal, and no other row have non-zero entries in that column. That is, with multiplied by its dagger, the entry on the diagonal will be $a^{\dagger}a = e^{\frac{-12{pi}i}{5}}e^{\frac{12{pi}i}{5}} = 1$, and all the other entries will be zero.
\item the same reasoning apply for labeling with the form ...pp*...  
\end{itemize}


\title{The Unitary of the Fibonacci representation (2) }
\begin{itemize}
\item with all the labelling with the form ...*p*... , the unitary row will include only one "b" in the matrix diagonal, and no other row have non-zero entries in that column. That is, with multiplied by its dagger, the entry on the diagonal will be $b^{\dagger}b = e^{\frac{-24{pi}i}{5}}e^{\frac{24{pi}i}{5}} = 1$, and all the other entries will be zero.
\item  with all the labelling with the form ...p*p.., the unitary row will include one "d" in the ...ppp.. entry, and one "c" in the ...p*p.. entry. that is, when multiplied by its dagger, the diagonal element will be $c^{\dagger}c + d^{\dagger}d = (A^{-8}\tau^{2} - A^{-4}\tau)(A^{8}\tau^{2} - A^{4}\tau) + (A^{-8}\tau^{\frac{3}{2}} + A^{-4}\tau^{\frac{3}{2}})(A^{8}\tau^{\frac{3}{2}} + A^{4}\tau^{\frac{3}{2}})$

$ = \tau^{4} - A^{-4}\tau^{3} - A^{4}\tau^{3} + \tau^{2} + \tau^{3} + A^{-4}\tau^{3} + A^{4}\tau^{3} + \tau^{3}$

$ = \tau^{4} + 2\tau^{3} + \tau^{2} = {\tau^{2}(\tau+1)}^{2} = 1^{2} = 1$ 
\end{itemize}


\title{The Unitary of the Fibonacci representation (3) }
\begin{itemize}
\item  now, lets check another entries in this row, after multiply this matrix by its dagger.
All the other rows has zero entries in the relevant columns, except the ...ppp.. row. when multiply, we will get $e^{\dagger}d + d^{\dagger}c = (A^{-8}\tau - A^{-4}\tau^{2})(A^{8}\tau^{\frac{3}{2}} + A^{4}\tau^{\frac{3}{2}}) + (A^{-8}\tau^{\frac{3}{2}} + A^{-4}\tau^{\frac{3}{2}})(A^{8}\tau^{2} - A^{4}\tau)$

$ = \tau^{2.5} + A^{-4}\tau^{2.5} - A^{4}\tau^{3.5} - \tau^{3.5} + \tau^{3.5} - A^{-4}\tau^{2.5} + A^{4}\tau^{3.5} - \tau^{2.5}$

$ = 0$
\item similar calculation will yield the result on the ...ppp... rows   
\end{itemize}

\subsection{Fibonacci representation and Jones Polynomial}
\title{Fibonacci representation and Jones polynomial}
We will want to connect somehow between the Fibonacci representation and the Jones Polynomial.
For this, we will use the Temperley-Lieb algebra, and the Markov Trace. We saw that the Markov trace
is a uniquely defined function over the Temperley-Lieb algebra, such that it is strongly connected to the Jones Polynomial. If we will be able to define a function over the Fibonacci representation that "behaves the same", we will be able to show such correlation between Jones Polynomial and the Fibonacci representation. We will donate this function as $\tilde{Tr}$


\title{Fibonacci representation and Jones polynomial (2)}
The requirements from  $\tilde{Tr}$ are:
\begin{itemize}
\item $\tilde{Tr}(1) = 1$
\item $\tilde{Tr}(XY) = \tilde{Tr}(YX)$
\item  $ \tilde{Tr}[xE_{n-1}]=\frac{\tilde(Tr)[x]}{d}$    
\end{itemize}
The last requirement, however, regards to a specific Temperley-Lieb element. Thus, we have to show that the Fibonacci representation and the Temperley-Lieb algebra "live in the same world", in order to translate $E_{n-1}$ to some matrix in the Fibonacci representation, and prove the requirement on the matrix after the translation. Therefore, in order to even start talking on $\tilde{Tr}$, we have to show a representation of the Temperley-Lieb algebra inside the Fibonacci representation.  


\title{Fibonacci representation and Jones polynomial (3)}
The requirements from the representation, are, as always, to preserve the original generators properties.
Here, the representation is: $\rho_{b}(E_{i}) = A^{-1}\rho_{F}(\sigma_{i}) - A^{-2}1$ , with 1 symbols the identity matrix.

it should hold that:
\begin{itemize}
\item $\rho_{b}(E_{i})\rho_{b}(E_{j}) = \rho_{b}(E_{j})\rho_{b}(E_{i}), when 2 \leq |i-j|$.This hold, since the Fibonacci representation is a representation, so $rho_{F}(\sigma_{i}),rho_{F}(\sigma_{j})$ commutes, and therefore $\rho_{b}(E_{i}),\rho_{b}(E_{j})$ commutes.
\item  ${\rho_{b}(E_{i}}^{2} = d\rho_{b}(E_{i}$ : in a similar way to what we did in the unitary proof, we can divide each  $rho_{F}(\sigma_{i}$ to blocks of
\[
\begin{pmatrix} a \end{pmatrix}
\begin{pmatrix} b \end{pmatrix}
\begin{pmatrix} c & d \\ d & e \end{pmatrix}
\], and prove that this relation hold on each block separately.  
\end{itemize}



\title{Fibonacci representation and Jones polynomial (4)}
The last requirement we should prove is that  
 $\rho_{b}(E_{i})\rho_{b}(E_{i+1})\rho_{b}(E_{i}) = \rho_{b}(E_{i})$)
 . Direct calculation shows that $rho_{b}(E_{1}), (rho_{b}(E_{2})$ with 3 strands satisfy this requirement. This suffice, since the relation between $rho_{b}(E_{i}), rho_{b}(E_{i+1})$ remains the same when i is change (up to re-index of the matrix, the matrix doesn't change), or when the number of strands get bigger with the same index (we will have more "squares" of the form mentioned in the previous slide for more equivalent labellings).


\title{Fibonacci representation and Jones polynomial (5)}
Its time to consider the specific function $\tilde{Tr}$, and prove its desired properties.
$\tilde{Tr} = \frac{1}{{\phi}f_{n}+f_{n-1}}\sum\limits_{s \in Q_{n+1}}{W_{s}}\rho_{f}(b)_{s,s}$,
when $\rho_{f}(b)_{s,s}$ denote the s-th diagonal entry in the Fibonacci representation of b,
and $W_{s}$ is $\phi$if s ends with p, and 1 if s ends with *. $Q_{n+1}$ is the set of all strings with length n+1 that obeys the "no two adjective *" rule. 

Its easy to see that $\tilde{Tr}(1) = 1$. In the identity matrix, all the diagonal entries equals to one, there are $f_{n}$ entries that end with p, and $f_{n-1}$entries that end with *, so 
$\tilde{Tr} = \frac{1}{{\phi}f_{n}+f_{n-1}}\sum\limits_{s \in Q_{n+1}}{W_{s}}\rho_{f}(b)_{s,s} =
 \frac{1}{{\phi}f_{n}+f_{n-1}} ({\phi}f_{n}+f_{n-1}) = 1$


\title{Fibonacci representation and Jones polynomial (6)}
Its easy to see that $\tilde{Tr}(XY) = \tilde{Tr}(YX)$, since at the end we talking about traces of matrices, and trace is a commutative property.

We remain with the last requirement of  $ \tilde{Tr}[xE_{n-1}]=\frac{\tilde(Tr)[x]}{d}$ . First, we will instantiate $E_{n-1}$ according to its representation: $E_{n-1} = A^{-1}\rho_{F}(\sigma_{n-1}) - A^{-2}1$. Therefore, $ \tilde{Tr}[xE_{n-1}] = \tilde{Tr}[A^{-1}x\rho_{F}(\sigma_{n-1}) - A^{-2}x]$

Therefore, from the linearity of the trace we get  $ \tilde{Tr}[xE_{n-1}] = A\tilde{Tr}[{\rho_{f}({\rho_{f}}^{-1}(x) * \sigma_{n-1})}] -A^{-2}\tilde{Tr}[x]$

Therefore, it remain to check  the value of $\tilde{Tr}[{\rho_{f}({\rho_{f}}^{-1}(x) * \sigma_{n-1})}]$

A careful examination of all the different labelling options of the new strand, combined with the Fibonacci representation rules for the new crossing, yield that this requirement indeed hold


\subsection{The Algorithm itself}
\title{The numbers representation}
We saw that the Fibonacci representation connects between braids and elements of p and *, such that the number of possible p,* sequences is $f_{n+2}$, when $f_{i}$ is the $i$th Fibonacci number.

If we where representing numbers in the "regular" form (binary basis - 1 for * and 0 for p, for example), then it would require n+1 qubits. 

However, the ratio $\frac{f_{i}}{2^{i}}$ becomes exponentially small (since $\frac{f_{i+1}}{f_{i}} < 2 $), and therefore computing the trace of the Fibonacci matrix over exponentially small number of elements, might not be in DQC1.

To resolve this issue, we will represent the elements {0...$f_{n+2}$} as $z(s) = \sum\limits_{i \in {1...n}}{s_{i}f_{i+1}}$.
It can be proven (by Induction), that this method indeed represents the numbers in the range {0...$f_{n+2}$}. 

\title{should we prove that?}


\title{The algorithm itself}
We will notice that in the new representation method, we require only $\log{f_{i+1}}$ bits, so the number of relevant bit strings from them is more then half. (because we can choose freely the */p value for at least half of the bytes).

We saw that computing the Jones polynomial at the point $e^{\frac{2{\pi}i}{5}}$ is equivalent to the Fibonacci representation trace estimation.
Given a certain braid, we can move cross-cross, and build its Fibonacci local matrix for each crossing (dimension 5x5), which dependent only on  neighbour strands, and leave all the rest in place. (we will extract those 3 bits from the rest, act on them locally, and then return the result to the strands). Multiplying all those circuits together will yield a quantum circuit which calculates the Fibonacci representation matrix, and then we can apply the Hadamard test for this matrix and get an approximation to the Jones polynomial.


\title{The algorithm itself (2)}
However, extracting those 3 bits from the rest and return them once the computation is done, might be a non-trivial issue, since we are not using the regular binary encoding (we have only $\log{f_{n+1}}$ to represent n+1 bits). 

However, we will note that we can, quite easily, calculate the value of the leftmost bit: The leftmost bit will be * $\iff f_{n+1} \leq z(s)$. But is computing two numbers can be done with one clean qubit?


\title{Arithmetic operations are in DQC1}
\begin{itemize}
\item We saw that the DQC1 power is the same as the power of circuit with logarithmically many clean qubits. 

\item the number of clean ancilla qubits that any arithmetic circuit will need is a constant!! (and is actually 3). we will prove that in the next few slides. 
\end{itemize}



\title{Arithmetic operations are in DQC1 (2)}
We first note that all the arithmetic operations are known to be in NC1, that is, they are known to have classical circuits with $O(\log{n})$ depth.

Intuitively, we are ready to "pay" in the final quantom circuit depth, in order to gain very narrow width of this circuit, so we will be able to remember only few ancilla bits.

A known theorem in the complexity theory field, states that this translation can be done easily. 



\title{Barrington's theorem}
Lets first define the term of "branching program", which is the depth-for-width optimization we discussed. Branching program is defined to be a set of $(G,s,t,\phi)$, when G is a DAG, s,t are start and end vertexes in the graph, and $\phi$ is an assignment from the input to the graph edges. The graph G is build in layers, when every node in each layer has up to two edges to node in the next layer. We define the "length" of such program to be the number of layers, and the "width" to be the maximal layer size. The branching program accepts iff there is a path from s to t under the assignment $\phi$ and the input x. 


\title{Barrington's theorem (2)}
For example, the following is 3-length and 2-width branching program, which gets $x_{1},x_{2},x_{3}$ as inputs, and accept if at least 2 of them are 1:
\begin{figure}
\includegraphics[scale=0.5]{majority} 
\caption{majority branching program}
\end{figure}.


\title{Barrington's theorem (3)}
Barrington's theorem shows, that any NC1-operation (include arithmetic ones), can be done with 5-width and poly-length branching program. For simulate such program on a quantom computer, we can label each "column" with the states $\ket{000}...\ket{100}$, and create simple unitary matrices for each branch, that extract the relevant qubit from the input, change the state of the 3 ancilla bits according to it. 

Start with 3 clean qubits at state 000, we expect to end in the same state at the end of this circuit process (and otherwise - we reject).


\title{The algorithm itself - closure}
\begin{itemize}
\item Concluding the above discussion, we can efficiently isolate the left-most bit using arithmetic operations.
\item however, we have to develop a method for saving all the other qubits as well. To isolate the $i$th qubit, we will first isolate the first qubit. if it was one, we will subtract $f_{n+1}$ from the input. Then, we can isolate the second-leftmost qubit by asking whether the remaining number is bigger then $f_{n}$ or not. If yes - the second qubit is *, and before isolate the third qubit we will subtract $f_{n}$, and so on... (The calculation of Fibonacci itself is of course in NC1, but it can be hard-coded into the circuit as well).
\end{itemize}






\section{The Hardness of computing Jones polynomial}
\subsection{The reduction from trace estimation to Jones polynomial evaluation - an overview}

We are now moving from showing an algorithm for estimate Jones polynomial of a trace closure of a braid at the point $\frac{2{\pi}i}{5}$, into showing that this is indeed a DQC1-hard problem. Specifically, we will want to show a reduction from trace-estimation to this estimation problem.

Say that we have some unitary matrix (with suitable quantom cicruit). How to reduce this problem to the Jones polynomials world?

We will have to construct some braid, of our choice, and show a correlation between of Jones polynomial and that circuit trace.

Since the only correlation we know so far between Jones Polynomial and quantom circuit is the Fibonacci representation, we will obviously have to approximate each quantom operation from the circuit C via Fibonnaci representation on some braid. For this, it is required to prove that Fibonacci representation matrices are dense, and we will prove that in subsection 3.2

More explicitly, we will show that $\{rho^{n}\}_{**}$ is dense subgroup of $SU(f_{n-1})$, and that $\{rho^{n}\}_{*p}$ is a dense subgroup of $SU(f_{n})$.

Next, we are tackeling the problem of the representation of qubit as braid with */p symbols. Since in our model there cannot be two adjacent $*$ symbols, a naive one-to-one mapping is not feasible.

Taking into account the fact that any such representation will have to give an "equal whight" to the different strings matching 0 and 1, and that we want to be able to operate on all the qubits freely, yields an optional encoding: $1 \rightarrow ppp, 0 \rightarrow p*p$.

However, there are many groups of three symbols, most of them aren't encode to anything, and we still want to know ahead what is the place that encode to each qubit abd qubit. We note that the probability that such group of three won't appear in $O(logn)$ bits is polynomially small. Therefore, we can assume that in every group of $O(\log{n})$ symbols we will find an encoding to one or to zero.

Notice that the Fibonacci matrix representation to $O(\log{n})$ bits is polynomial, so we will be able to actually construct these matrixes.

By the density theorem which will be proven in section 3.2, we now that there is some braid that suit to that matrix circuit. However, the number of crossing in the braid might be large. In section 3.3 we will prove that $poly(n)$ crossings in that braid is enough for any level of approximation that we want.

However, two issues are still remain.

First, we will show that Fibonacci matrices that act on string which start at $*$ are dense. But what we do in the case that there is no $*$ around?. Note that for the first 2 blocks, we can assume that there is a "*" in the middle of them (since it happens with a constant probability, we can make the other options to not contribute to the final trace, similar to what we did in section 1, with ancilla qubits and CNOTs). However, this method cannot be applied to all the blocks (since we will need polynomial number of ancila qubits, which we cannot have in DQC1 reduction).

Therefore, a method one can try is to "move the $*$". Specifically, we can imply a unitary transformation that "switches" between two segments in the center of two neigthboring blocks, if they are both in the same size, and both starts and end with a "p". (Since its just a replace action - its obvious that this matrix is indeed unitary). Note that:
\begin{itemize}
\item Since this matrix act on two blocks, its size is still polynomial.
\item Using this matrix we can bring a "*" to every block: At the start we have * in the middle of the first two blocks. Using this matrix once we can move the "*" to be in the first and the third blocks. Use it again, and we can move the "*" to be in the center of the second and the third blocks, and etc.
\item The probability that we wont find such "p" segment to make the replace on is polynomially small. Furthermore, the probability that our encoding lies within these segments is also polynomially small. Thus, we can assume that these conditions always hold, and add polynomially small error to the calculations. 
\end{itemize}

The second issue that we have to take into account, is the fact that the weight that the trace gives to each encoding is the same, while the weight that the Jones polynomial calculation gives is not. This issue can be solved by adding  one more superblock at the end. This superblock encoding will match the encoding of a matrix that, conditioned on the last original bit (i.e. just if it was "p"), rotate the state, so the inner product of the new state with the old state is exactly $\frac{1}{\phi}$ (recall that in the Jones polynimial calculations, the weight we gave to the "ending with p states" was $\phi$). The orthogonal part of the state after the rotation will obviously wont contribute to the trace, and therefore we will get in the Jones polynomial calculation that its value is equal to the trace in the original circuit.

This completes the proof that the Jones polynomial estimation is hard in DQC1 (except proving the theoremes in sections 3.2 and 3.3). 
\subsection{Fibonacci representation gates are dense}
In this section we will prove that the Fibonacci representation gates are dense. Specifically, we will prove that:
\begin{itemize}
\item ${\rho^{n}}_{**}$ (which stand for all the Fibonacci matrices, taken over the subspace of strings with start and end with *), are dense in $SU(f_{n-1})$
\item ${\rho^{n}}_{*p}$ (which stand for all the Fibonacci matrices, taken over the subspace of strings with start and end with *), are dense in $SU(f_{n})$
\end{itemize}

Together, this two statements implies the fact that as long that our encoding start with "*", we can find a suitable Fibonacci matrix that will approximate any unitary transformation on our n bits.

First, lets remind what the term "dense" means. 
\title{density- definition}
A subset of gates $\in SU(f_{n+1})$ is said to be dense, if we can approximate any gate in $ SU(f_{n+1})$ by this subset. Formally, we can measure the distance between 2 unitary matrices by their trace distance: 
$D(\sigma, \rho) = \frac{1}{2}Tr[\sqrt{(\rho - \sigma)^{\dagger}(\rho - \sigma)}]$
, and define a set of gates to be $\epsilon$-dense if for every unitary matrix there is a gate from the group (or can be built by the group) which is $\epsilon$ close to it.

We will prove that therorem by induction on n. The base case is to prove that ${\rho^{4}}_{**}$, ${\rho^{3}}_{*p}$ are dense in $SU(2)$. How to prove density in $SU(2)$?

For that we will use an isomorphism from $SU(2)$ to $SO(3)$ - the group of rotations on the sphere with radius 1.


\subsubsection{SO(3) and the connection to density}

\title {The SO(3) group and its subgroups}
The SO(3) group is defined to be the rotation group in the unity circle. That is, each element in SO(3) is defined by its rotation degree $\theta$ and by its axis or rotation (x,y,z). Note that we can easily find isomorphism $\phi$ from the single qubit operations SU(2) to the rotation group: we can write each 2x2 unitary matrix as $cos(\frac{\theta}{2}I + \frac{\theta}{2}{a\sigma_{x} + b\sigma_{y} + c\sigma_{z}})$, which correlates each unitary operation to a degree $\theta$ and an axis of (a,b,c). For a general group of gates G, it is obvious that if ${\phi(x) for x \in G}$ does not contained in any finite subgroup of SO(3), then it will actually be able to create ALL SO(3), which means that after the isomorphism the results will be "dense in SO(3)", which will make the original kernel dense in SU(2).

Therefore, we have to determine what the finite subgroups of $SO(3)$ looks like.

\begin{itemize}
\item Consider such subgroup $G \subset SO(3)$. Each element $g \in G$ is an action that rotates some point on a sphere, to some other point on that sphere. 
\item Since its a rotation, each action should have 2 "poles" - points which stay invariant after the rotation is taking place.
\item We will say that two poles $p, p^{'}$ are equivalent, if there exist some action $g \in G$ such that $g \circ p = p^{'}$, and noate this classes by $C_{1}...C_{M}$.
\item Note that if $p, p^{'}$ are equivalent with some action $g$, then any action $h$ which leaves $p$ in place, can be used to obtained an action $g \circ h$ which send $p$ to $p^{'}$.
\item In addition, note that neceessarily $p$ and $p^{'}$ stay invarant under the same number of operations, since ($g, h$ defined as before) $ghg^{-1}$ is an action which $p^{'}$ is its pole: $ghg^{-1}p^{'} = ghp =gp = p^{'}$.
\item from all the above, we get the relation: $|C_{i}| = \frac{N}{n_{i}}$, when $n_{i}$ is the number of operations that leaves some pole $p$ in place, and N is the number of elements in G.

\item In addation, note that $\sum\limits_{1..M}{C_{i}(n_{i} -1)} = 2(N-1)$, since the left side contains the number of poles, when each pole is multiplied by the non-trivial elements that leaves it fixed, and any element from the $N-1$ non trivial elements has 2 poles.

\item the combination of the two formulas from abobe yields the following restriction on the SO(3) subgroups: $\sum\limits_{1..M}{1 - \frac{1}{n_{i}}} = 2(1-\frac{1}{N})$.

\item This solution has only 5 options:
\item $M=2, n_{1}=n_{2}=N$. this solution describes a subgroup of size N, contains cyclic rotation of $\frac{2{\pi}i}{N}$ around the same circle.
\item $M=3, n_{1}=n_{2}=2, n_{3}=\frac{N}{2}$. This solution describes a dehideral group (imagine a regular cyclic group with size N, that we add to it all the reflections of each element as well).
\item $M=3, n_{1}=2, n_{2}=n_{3}=3, N=12$, describes a tetrahedron.
\item $M=3, n_{1}=2,n_{2}=3, n_{3}=4, N=12$, descrobes octahedron,
\item $M=3, n_{1}=2, n_{2}=3, n_{3}=5, N=60$, decribes dedocahedron.
\end{itemize}

Notice that in the last 3 finite subgroups, the largest order possible for some element is 5. We will use this fact later. 

Now, after we know that, lets return to the density therorem proof.

As for the ${\rho^{4}}_{**}$ case, its defined 2 matrices:

${\rho^{4}}_{**}(\sigma_{1}) = {\rho^{4}}_{**}(\sigma_{3}) = \begin{pmatrix} b & 0 \\ 0 & a \end{pmatrix}$

${\rho^{4}}_{**}(\sigma_{2}) =  \begin{pmatrix} c & d \\ d & e \end{pmatrix}$

Since global phase does not interent us, we can normalize these matrices to have determinant of one, thus being a subgroup of $SU(2)$. we get the matrices:

$A = \frac{1}{\sqrt{ab}} \begin{pmatrix}  b & 0 \\ 0 & a \end{pmatrix}$
and:
$B = \frac{1}{\sqrt{ce - d^{2}}}  \begin{pmatrix} c & d \\ d & e \end{pmatrix}$

after imply the isomorphism $\phi$ on these matrices, we get a rotation by angle of $\frac{7{\phi}}{5}$, by different axes. The angle between these axes is $\theta_{12} = cos^{-1}(2 - \sqrt{5}) = 1.809$.

All that remain is to show that this subgroup of $SU(3)$ is not contained in any final subgroup. First, note that this subgroup has two diferent rotation axes, therefore it cant be contained in one of the first two options, for every N. Second, note that ${\phi(A)}^{5}{\phi(B)}^{5}$ is a rotation by $2\theta_{12}$. (This is because ofter 5 rotations of $\phi(A)$ we get rotation by $7\phi$ arount the first axe (which is equal to the "mirror" around to this axe, and after 5 rotations more of $\phi(B)$ we will get a mirror around the second axe - similar to whats going on in the groover algorithm). Direct checking shows that this rotation order in $SO(3)$ is more then 3, therefore  it cant be contained in any of the last 3 finite groups. This completes the proof that  ${\rho^{4}}_{**}$ is dense in $SU(2)$ (up to a phase). 

As a result, the fact that ${\rho^{3}}_{*p}$ is dense in  $SU(2)$ immediatly follows (the two matrices of ${\rho^{3}}_{*p}$  are identical to the two matrices of ${\rho^{4}}_{**}$ ).

We are now moving to prove the induction step. First, we take a closer look at  ${\rho^{n}}_{**}$. We note that we can split each matrix in ${\rho^{n}}_{**}$ to two matrices - one that act on the subspace ending with *p* like the original matrix, and one that act on the subspace pp* like the original matrix. By definition $\rho^{n}_{**}(\sigma_{1}) ... \rho^{n}_{**}(\sigma_{n-3})$ will not "mix" these subspaces.

Furthermore, since each $\sigma_{i}$ like this affects only the neigboring "bits", it holds that there is one to one correspondense between $\rho^{n-2}_{**}(\sigma_{1}) ... \rho^{n-2}_{**}(\sigma_{n-3})$ and $\rho^{n}_{*...*p*}(\sigma_{1}) ... \rho^{n-2}_{*...*p*}(\sigma_{n-3})$, 

and one to one correspondence between $\rho^{n-1}_{**}(\sigma_{1}) ... \rho^{n-1}_{**}(\sigma_{n-3})$ and $\rho^{n}_{*...pp*}(\sigma_{1}) ... \rho^{n}_{*...pp*}(\sigma_{n-3})$ .

However, $\rho^{n}_{**}(\sigma_{n-2})$ will mix these two subspaces, since it will affect the $n-1$ symbol.

We will inroduce two lemmas that will help us proving, that the above analysis actually implies that ${\rho^{n}}_{**}$ is dense.

The first lemma that we need is the bridge lemma: we saw that the subspace ** can be devided to two subspaces, when we know that  ${\rho^{n-1}}_{**}$ and ${\rho^{n-2}}_{**}$ are dense in them. We also saw that $\rho^{n}_{**}(\sigma_{n-2})$ mixes these subspaces. We will want to show that it means something about $\rho^{n}_{**}$ density in $SU(f_{n-1})$. 

\subsubsection{The bridge lemma}
Consider a linear space C which is a direct sum of two orthogonal subspaces A and B, and assume that dimB > dimA  1.
Let W be a bridge transformation between A and B. Then any U  SU(C) can be
approximated to an arbitrary precision using a finite sequence of transformations from
SU(A), SU(B) and W. Consequently, the group generated by SU(A), SU(B) and W
is dense in SU(C).

Here A and B are of course $SU(f_{n-3}), SU(f_{n-2})$, C is $SU(f_{n-1})$, and the bridge element is  $\rho^{n}_{**}(\sigma_{n-2})$.

However this is not enough! We indeed know that the space  $SU(f_{n-1})$ can be devided to two parts, such that  $\rho^{n-2}_{**},  \rho^{n-2}_{**}$ are dense in them.... but after we "combine" these operations together, we might get some dependecy: i.e, we will be able to get every matrix from $SU(f_{n-3})$, but the matices we will use will prevent us from get some of the matrices in $SU(f_{n-2})$ (and vice versa...).

To resolve that issue, we will have to inroduce the decoupeling lemma:

However, this is still not enough: indeed, we now that 
\subsubsection{The decoupling lemma}
Let G be an infinite discrete group, and let A, B be two finite Linear spaces with different dimensionality. Let $\tau_{a}$ and $\tau_{b}$ be
two homomorphisms of G into SU(A) and SU(B) respectively and assume that $\tau_{a}(G)$
is dense in SU(A) and $\tau_{b}(G)$ is dense in SU(B). Then for any U  SU(A) there exist
a series {n} in G such that
$\tau_{b}(n)  U$
$\tau_{b}(n)  1$


Using this final lemma we will be able to finally complete the proof of the density of  $\rho^{n}_{**}$:

Base case: clearly $\rho^{3}_{**}$ is dense in $SU(1)$ (which include only one matrix).
            We showed that $\rho^{4}_{**}$ is dense in $SU(2)$
            
Step: Assume that  $\rho^{n-2}_{**}(B_{n-1})$ is dense in $SU(f_{n-3})$, and  $\rho^{n-2}_{**}(B_{n-1})$ is dense in $SU(f_{n-2})$, and we would like to prove that  $\rho^{n}_{**}(B_{n})$ is dense in $SU(f_{n-1})$. By the fact observed above $\rho^{n-2}_{**}(\sigma_{1}) ... \rho^{n-2}_{**}(\sigma_{n-3})$ remains unchanged when moving to  $\rho^{n}_{**}$. Therefore, by the induction assumption and the decoupeling lemma, using $\rho^{n}_{**}$ we can approximate any $U \oplus 1$ when $U \in SU(f_{n-3})$, and any  $1 \oplus U$ when $U \in SU(f_{n-2})$,

By the bridge lemma, this fact (and the fact that we use  $\rho^{n}_{**}(\sigma_{n-2})$ as a "bridge"), is enough for claiming that  $\rho^{n}_{**}(B_{n})$ is dense in $SU(f_{n-1})$.

\end{document}

